In today’s class, we discussed why Big Data technologies are required and how traditional data systems fail when data grows to the scale of terabytes and petabytes. Since such large volumes of data cannot be processed efficiently on a single system, we explored how distributed data processing becomes essential.
We first learned about Hadoop architecture, specifically HDFS (Hadoop Distributed File System). In Hadoop, large files are broken into 128 MB blocks and distributed across multiple systems called DataNodes. The NameNode acts as the master and stores metadata about where each file block is located, while the DataNodes act as slaves that store the actual data. This follows a master–slave architecture.
Within Hadoop, data processing happens using MapReduce. The map() function processes and indexes input data (such as words in a text), and the reduce() function aggregates the results by identifying repeated values and summarizing them. We discussed how MapReduce operations rely heavily on disk-based storage (HDFS), which makes Hadoop reliable but slow due to disk I/O operations.
One major advantage of Hadoop that we discussed is data replication. Each data block is stored in three copies across different DataNodes, ensuring fault tolerance and data safety. This makes Hadoop highly suitable for banking and financial sectors, where data loss is unacceptable. However, the drawback of Hadoop is its slow processing speed, which led to the development of Spark.
Next, we discussed Spark architecture, which was designed to overcome Hadoop’s performance limitations. Spark is a unified in-memory computation framework, meaning most processing happens in RAM instead of disk. This significantly reduces I/O time and makes Spark much faster.
Spark consists of a Driver Program, a Cluster Manager (such as YARN or Kubernetes), and Worker Nodes (Executors). Spark uses RDDs (Resilient Distributed Datasets), which allow data to be processed in memory while still being fault tolerant. Because of this in-memory processing model, Spark is ideal for real-time and high-speed computations.
We compared Hadoop and Spark and discussed real-life use cases. For example, in fraud detection, where immediate action is required (such as OTP hacking), Spark is preferred because Hadoop’s disk-based processing would be too slow. This helped clarify why Spark is commonly used in real-time analytics, while Hadoop is better suited for batch processing and long-term data storage.
We also discussed data security concepts, including data masking, encryption at rest, and encryption in transit using protocols like SSL and TLS. Additionally, we covered the importance of data quality and testing, which analysts perform to maintain data integrity.
Towards the end of the class, we discussed the data consumption layer, where tools such as Power BI and Tableau are used for reporting and visualization. We also learned that production data is often exposed through APIs like FastAPI or Flask, and why it is important to keep development and production environments separate.
Finally, we briefly discussed real-time data engineering and tools like Kafka and Kinesis. Since these are paid cloud services, we were advised to understand them through videos and architectural explanations for interview preparation.
Overall, today’s class helped me clearly understand the difference between Hadoop and Spark, their architectures, advantages, limitations, and real-world applications. I learned how modern data engineering systems balance data safety, speed, and scalability, and why Spark has become the preferred choice for real-time data processing today.
At Last we were also asked to install and download My SQL, SQL Worksheet and Sakila Database.

